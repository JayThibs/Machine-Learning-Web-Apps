{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Automated Medical Diagnosis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayThibs/Machine-Learning-Web-Apps/blob/master/Automated-Medical-Diagnosis/Automated_Medical_Diagnosis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijE5YRfZev2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os # for reading from the file\n",
        "import math # for any kind of data preprocessing\n",
        "import json # for reading data\n",
        "import numpy\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3iMCCaPfonQ",
        "colab_type": "text"
      },
      "source": [
        "*enable_eager_exection*: there are two types of machine learning\n",
        "\n",
        "1.   Define **by** Run (PyTorch): THE GOOD WAY TO BUILD MODELS. Faster, dynamic, more resilient, easier to train, and faster to train. It performs operations as you create them instead of waiting until compile time.\n",
        "2.   Define **and** Run (Keras)\n",
        "\n",
        "PyTorch was the framework that made Define **by** Run popular. TensorFlow is not Define **and** Run by default, but you can use tf.enable_eager_execution() to make it use Define **by** Run.\n",
        "\n",
        "Eager Execution is particularly important when we are doing transfer learning.\n",
        "\n",
        "AUTOTUNE makes the model train faster.\n",
        "\n",
        "Optimizing Performance not only involves making the model train as fast as possible but also making the data pipeline as fast as possible. Without pipelining, the CPU and the GPU/TPU sit idle for much of the time. The idle time is cut significantly with pipelining. We use AUTOTUNE to make sure the batch process happens as fast as possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yoc2jTzuf1Um",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.enable_eager_execution()\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6mBXazOi-n2",
        "colab_type": "text"
      },
      "source": [
        "GCS is way to track where our file is "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuIvtMmPjIgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GCS_PATTERN = 'gs://flowers-public/tfrecords-jpg-192x192-2/*.tfrec'\n",
        "IMAGE_SIZE = [192,192]\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "VALIDATION_SPLIT = 0.19\n",
        "CLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
        "\n",
        "filenames = tf.gfile.Glob(GCS_PATTERN)\n",
        "split = int(len(filenames) * VALIDATION_SPLIT)\n",
        "training_filenames = filenames[split:]\n",
        "validation_filenames = filenames[:split]\n",
        "validation_steps = 30\n",
        "steps_per_epochs = 30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_IonO2okj14",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}